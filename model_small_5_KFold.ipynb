{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86f22f3-1e5b-48bb-8c52-34bcf1677455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April RPI: 1.0\n",
      "May RPI: 1.5\n",
      "June RPI: 1.0\n",
      "July RPI: 2.5\n",
      "August RPI: 3.25\n",
      "September RPI: 2.25\n",
      "October PRI: 1.5\n",
      "9728\n",
      "(224, 224, 3)\n",
      "9728\n",
      "[[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]\n",
      "=============================\n",
      "625\n",
      "(224, 224, 3)\n",
      "625\n",
      "[[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_57 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m221\u001b[0m, \u001b[38;5;34m221\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m784\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_76 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_58 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │         \u001b[38;5;34m1,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_77 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_59 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │           \u001b[38;5;34m292\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_78 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m4\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_79 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m4\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_19 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,879</span> (11.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,879\u001b[0m (11.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,879</span> (11.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,879\u001b[0m (11.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Training fold 1...\n",
      "Epoch 1/3\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.4282 - loss: 3.1290\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53660, saving model to 5_best_model_small_Kfold_2.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 196ms/step - accuracy: 0.4286 - loss: 3.1184 - val_accuracy: 0.5366 - val_loss: 1.0038\n",
      "Epoch 2/3\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5206 - loss: 1.0133\n",
      "Epoch 2: val_accuracy improved from 0.53660 to 0.55613, saving model to 5_best_model_small_Kfold_2.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 192ms/step - accuracy: 0.5207 - loss: 1.0132 - val_accuracy: 0.5561 - val_loss: 0.9640\n",
      "Epoch 3/3\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.5472 - loss: 0.9715\n",
      "Epoch 3: val_accuracy improved from 0.55613 to 0.64988, saving model to 5_best_model_small_Kfold_2.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 210ms/step - accuracy: 0.5473 - loss: 0.9713 - val_accuracy: 0.6499 - val_loss: 0.8656\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for fold 2:\n",
      "[[2170    0  413]\n",
      " [ 750    0   65]\n",
      " [ 475    0  991]]\n",
      "Training fold 2...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/future-outlier/miniconda3/envs/dev/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.4965 - loss: 3.4467\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56312, saving model to 5_best_model_small_Kfold_3.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 213ms/step - accuracy: 0.4967 - loss: 3.4345 - val_accuracy: 0.5631 - val_loss: 0.9113\n",
      "Epoch 2/3\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.5842 - loss: 0.8940\n",
      "Epoch 2: val_accuracy improved from 0.56312 to 0.64556, saving model to 5_best_model_small_Kfold_3.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 214ms/step - accuracy: 0.5842 - loss: 0.8940 - val_accuracy: 0.6456 - val_loss: 0.7953\n",
      "Epoch 3/3\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.6404 - loss: 0.7740\n",
      "Epoch 3: val_accuracy improved from 0.64556 to 0.65090, saving model to 5_best_model_small_Kfold_3.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 174ms/step - accuracy: 0.6404 - loss: 0.7740 - val_accuracy: 0.6509 - val_loss: 0.7448\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for fold 3:\n",
      "[[2318   21  138]\n",
      " [ 505  368    6]\n",
      " [1020    8  480]]\n",
      "Average Confusion Matrix:\n",
      "[[2244.    10.5  275.5]\n",
      " [ 627.5  184.    35.5]\n",
      " [ 747.5    4.   735.5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "average_test_predictions: [[0.4695348  0.09646176 0.43400344]\n",
      " [0.6670524  0.08768697 0.24526064]\n",
      " [0.6073727  0.1217714  0.27085584]\n",
      " ...\n",
      " [0.5633478  0.0973707  0.33928147]\n",
      " [0.65636146 0.11220022 0.23143834]\n",
      " [0.5313038  0.07650305 0.39219314]]\n",
      "predicted_classes_index: [0 0 0 2 0 0 2 0 0 2 0 0 2 0 0 0 0 0 0 2 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 2 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 2 2 0 0 0 2 0 0 2 2 2 0 0 0\n",
      " 0 2 0 0 2 2 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 2 2\n",
      " 0 2 0 2 0 0 2 0 0 0 0 0 2 2 0 0 0 0 2 0 0 0 2 0 0 0 2 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 0 0 0 2 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 0 2 0 0 0 2 0 0 0 2 2 0\n",
      " 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 2\n",
      " 0 0 2 1 0 0 0 0 0 0 2 2 0 2 0 0 0 0 2 0 0 0 0 0 2 0 0 2 0 2 0 0 0 0 0 2 0\n",
      " 0 2 2 0 2 0 0 0 0 0 0 2 0 0 0 2 0 2 0 2 0 0 0 2 2 0 0 0 0 2 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 2 0 0 0 0 0 2 0 2 0 0 0 0 0 0 2 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 2 0 0 2 0 0 0 0 2 2\n",
      " 0 0 0 0 2 0 2 0 0 0 0 2 0 0 0 0 0 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0\n",
      " 2 0 2 2 2 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 2 2 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 2 0 0 0 2 0 0 2 2 0 0 0 0 2 0 2 2 2\n",
      " 2 0 2 2 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 2 2 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 2 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 2 0 2 0 0 0 2 2 0 0 0 0 0]\n",
      "Confusion Matrix:\n",
      "[[490   1 134]\n",
      " [  0   0   0]\n",
      " [  0   0   0]]\n",
      "Accuracy: 0.784\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential, layers, Input\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.initializers import RandomNormal, HeNormal, GlorotNormal\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#溶氧量點數\n",
    "def DO_value(num):\n",
    "  if (num >= 6.5):\n",
    "    return 1\n",
    "  elif(num >= 4.6):\n",
    "    return 3\n",
    "  elif(num >= 2.0):\n",
    "    return 6\n",
    "  else:\n",
    "    return 10\n",
    "\n",
    "#懸浮固體點數\n",
    "def TSS_value(num):\n",
    "  if (num <= 20.0):\n",
    "    return 1\n",
    "  elif(num <= 49.9):\n",
    "    return 3\n",
    "  elif(num <= 100):\n",
    "    return 6\n",
    "  else:\n",
    "    return 10\n",
    "\n",
    "#生化需氧量點數\n",
    "def BOD_value(num):\n",
    "  if (num <= 3.0):\n",
    "    return 1\n",
    "  elif(num <= 4.9):\n",
    "    return 3\n",
    "  elif(num <= 15.0):\n",
    "    return 6\n",
    "  else:\n",
    "    return 10\n",
    "\n",
    "#氨氣點數\n",
    "def NH3N_value(num):\n",
    "  if (num <= 0.5):\n",
    "    return 1\n",
    "  elif(num <= 0.99):\n",
    "    return 3\n",
    "  elif(num <= 3.0):\n",
    "    return 6\n",
    "  else:\n",
    "    return 10\n",
    "\n",
    "#計算RPI(River Pollution Index)\n",
    "def cal_RPI(DO, TSS, BOD, NH3N):\n",
    "  DO_num = DO_value(DO)\n",
    "  TSS_num = TSS_value(TSS)\n",
    "  BOD_num = BOD_value(BOD)\n",
    "  NH3N_num = NH3N_value(NH3N)\n",
    "  return (DO_num + TSS_num + BOD_num + NH3N_num)/4\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# RPI = (DO + SS + BOD + NH3N) / 4\n",
    "# NH3-N / NO3-N = K  -->  NH3-N = NO3-N * K\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# def cal_RPI(DO, SS, BOD, NH3N):\n",
    "#     return (DO + SS + BOD + NH3N) / 4\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "APRIL_RPI = cal_RPI(9.0 ,15, 0 ,1.3/4.5)\n",
    "MAY_RPI = cal_RPI(8.4, 10, 0, 2.5/4.5)\n",
    "JUNE_RPI = cal_RPI(8.5, 16, 0, 1.7/4.5)\n",
    "JULY_RPI = cal_RPI(6.4, 24, 0, 2.6/4.5)\n",
    "AUGUST_RPI = cal_RPI(8.1, 143, 0, 1.2/4.5)\n",
    "SEPTEMBER_RPI = cal_RPI(7.7, 53, 0, 1.5/4.5)\n",
    "OCTOBER_RPI = cal_RPI(7.8, 23, 0, 1.2 / 4.5)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# Printing the calculated values\n",
    "print(\"April RPI:\", APRIL_RPI)\n",
    "print(\"May RPI:\", MAY_RPI)\n",
    "print(\"June RPI:\", JUNE_RPI)\n",
    "print(\"July RPI:\", JULY_RPI)\n",
    "print(\"August RPI:\", AUGUST_RPI)\n",
    "print(\"September RPI:\", SEPTEMBER_RPI)\n",
    "print(\"October PRI:\", OCTOBER_RPI)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# todo: change the label\n",
    "def get_label(val):\n",
    "    if val <= 2.0:\n",
    "        return \"NP\"\n",
    "    elif val <= 3.0:\n",
    "        return \"SP\"\n",
    "    elif val <= 6.0:\n",
    "        return \"MP\"\n",
    "    else:\n",
    "        return \"SEVERE\"\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "folder_paths = ['./水質檢測/Image_data/1_April',\n",
    "                './水質檢測/Image_data/2_May',\n",
    "                './水質檢測/Image_data/3_June',\n",
    "                './水質檢測/Image_data/4_July',\n",
    "                './水質檢測/Image_data/5_August',\n",
    "                './水質檢測/Image_data/6_September']\n",
    "labels = [get_label(APRIL_RPI), get_label(MAY_RPI), get_label(JUNE_RPI), \n",
    "          get_label(JULY_RPI), get_label(AUGUST_RPI), get_label(SEPTEMBER_RPI)]  # 对应文件夹1和文件夹2的标签\n",
    "\n",
    "\n",
    "oct_paths = [\"./水質檢測/Image_data/7_October\"]\n",
    "oct_label_Stirng = [get_label(OCTOBER_RPI)]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def load_and_label_images(folder_path, label):\n",
    "    image_paths = []  # 存储图像文件的路径\n",
    "    labels = []       # 存储图像对应的标签\n",
    "\n",
    "    # 获取文件夹中的所有图像文件的路径\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  # 假设只加载jpg和png格式的图像文件\n",
    "            image_paths.append(os.path.join(folder_path, filename))\n",
    "            labels.append(label)\n",
    "\n",
    "    # 加载图像并为其指定标签\n",
    "    images = [tf.io.read_file(image_path) for image_path in image_paths]\n",
    "    images = [tf.image.decode_image(image, channels=3) for image in images]\n",
    "\n",
    "    # 可选的数据预处理：这里假设对图像进行归一化\n",
    "    #images = [tf.cast(image, tf.float32) / 255.0 for image in images]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# 你可以根据需要加载多个文件夹中的图像并为其指定不同的标签\n",
    "# folder_paths = ['/path/to/folder1', '/path/to/folder2']\n",
    "# labels = [1, 2]  # 对应文件夹1和文件夹2的标签\n",
    "\n",
    "# 加载所有文件夹中的图像和标签\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for folder_path, label in zip(folder_paths, labels):\n",
    "    images, labels = load_and_label_images(folder_path, label)\n",
    "    all_images.extend(images)\n",
    "    all_labels.extend(labels)\n",
    "    \n",
    "\n",
    "oct_images = []\n",
    "oct_labels = []\n",
    "for oct_path, label in zip(oct_paths, oct_label_Stirng):\n",
    "    images, labels = load_and_label_images(oct_path, oct_label_Stirng)\n",
    "    images_resized = [tf.image.resize(img, (224, 224)) for img  in images]\n",
    "    oct_images.extend(images_resized)\n",
    "    oct_labels.extend(labels)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# print(all_labels)\n",
    "# print(oct_labels)\n",
    "\n",
    "label_mapping = {\n",
    "    \"NP\":[1,0,0],\n",
    "    \"MP\":[0,1,0],\n",
    "    \"SP\":[0,0,1]\n",
    "}\n",
    "\n",
    "# unique_labels_list = list(set(all_labels))\n",
    "# unique_labels_list.sort()\n",
    "# label_to_index = {label: index for index, label in enumerate(unique_labels_list)}\n",
    "# label_one_hot = [tf.one_hot(label_to_index[label], len(unique_labels_list)) for label in all_labels]\n",
    "label_one_hot = [label_mapping[label] for label in all_labels]\n",
    "oct_label_one_hot = [label_mapping[label[0]] for label in oct_labels]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "print(len(all_images))\n",
    "print(all_images[0].shape)\n",
    "print(len(label_one_hot))\n",
    "print(label_one_hot[0:10])\n",
    "\n",
    "print(\"=============================\")\n",
    "\n",
    "print(len(oct_images))\n",
    "print(oct_images[0].shape)\n",
    "print(len(oct_label_one_hot))\n",
    "print(oct_label_one_hot[0:10])\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# train_images, validation_images, train_labels, validation_labels = train_test_split(all_images, label_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 创建训练集数据集\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=len(train_images)).batch(batch_size=32)\n",
    "\n",
    "# # 创建验证集数据集\n",
    "# validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "# validation_dataset = validation_dataset.shuffle(buffer_size=len(validation_images)).batch(batch_size=32)\n",
    "\n",
    "# 創建測試資料集\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((oct_images, oct_label_one_hot))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=len(oct_images)).batch(batch_size=32)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# # 构建数据集\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((all_images, label_one_hot))\n",
    "\n",
    "# # 可选的打乱和分批处理\n",
    "# dataset = dataset.shuffle(buffer_size=len(all_images)).batch(batch_size=32)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 从数据集中获取并显示图像\n",
    "# fig, axs = plt.subplots(2, 5, figsize=(20, 8))  # 创建一个2x5的子图布局\n",
    "\n",
    "# for i, (image, label) in enumerate(dataset.take(10)):  # 取出前十张图像\n",
    "#     row = i // 5  # 计算当前图像应该位于的行索引\n",
    "#     col = i % 5   # 计算当前图像应该位于的列索引\n",
    "\n",
    "#     axs[row, col].imshow(image[0])  # 假设每个batch里只有一张图像\n",
    "#     axs[row, col].set_title('Label: {}'.format(label[0].numpy()))\n",
    "#     axs[row, col].axis('off')  # 关闭坐标轴\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# 定义一个简单的卷积神经网络模型\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (4, 4), activation='relu', input_shape=(224, 224, 3)),  # 減少過濾器數量\n",
    "        tf.keras.layers.MaxPooling2D((4, 4)),\n",
    "        tf.keras.layers.Conv2D(8, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((4, 4)),\n",
    "        tf.keras.layers.Conv2D(4, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),  # 減少全連接層大小\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "print(create_model().summary())\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "num_fold = 2\n",
    "all_images = np.array(all_images)\n",
    "label_one_hot = np.array(label_one_hot)\n",
    "\n",
    "kf = KFold(n_splits=num_fold, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "confusion_matrices = []\n",
    "\n",
    "for train_index, val_index in kf.split(all_images):\n",
    "    print(f\"Training fold {fold}...\")\n",
    "    # print(f\"train_index={train_index}, val_index={val_index}\")\n",
    "    fold += 1\n",
    "    \n",
    "    train_images, val_images = all_images[train_index], all_images[val_index]\n",
    "    train_labels, val_labels = label_one_hot[train_index], label_one_hot[val_index]\n",
    "    \n",
    "    # 創建數據集\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(train_images)).batch(32)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "    val_dataset = val_dataset.batch(32)\n",
    "    \n",
    "    # 創建和編譯模型\n",
    "    model = create_model()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # 訓練模型\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=3, callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=f'5_best_model_small_Kfold_{fold}.keras', \n",
    "                                           monitor='val_accuracy', \n",
    "                                           save_best_only=True, \n",
    "                                           mode='max', \n",
    "                                           verbose=1)\n",
    "    ])\n",
    "    \n",
    "    # 評估模型\n",
    "    model.load_weights(f'5_best_model_small_Kfold_{fold}.keras')\n",
    "    \n",
    "    # 使用模型預測\n",
    "    predictions = model.predict(val_dataset)\n",
    "    predicted_classes_index = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    true_classes_index = np.argmax(val_labels, axis=1)\n",
    "    \n",
    "    # 計算混淆矩陣\n",
    "    conf_matrix = confusion_matrix(true_classes_index, predicted_classes_index)\n",
    "    confusion_matrices.append(conf_matrix)\n",
    "    \n",
    "    print(\"Confusion Matrix for fold {}:\".format(fold))\n",
    "    print(conf_matrix)\n",
    "    model.save(f\"5_model_small_Kfold_{fold}.h5\")\n",
    "\n",
    "# 平均混淆矩陣\n",
    "average_conf_matrix = np.mean(confusion_matrices, axis=0)\n",
    "print(\"Average Confusion Matrix:\")\n",
    "print(average_conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "all_test_predictions = []\n",
    "\n",
    "for fold in range(2, num_fold+2):\n",
    "    model_name=f\"5_model_small_Kfold_{fold}.h5\"\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    fold_predictions = model.predict(test_dataset)\n",
    "    all_test_predictions.append(fold_predictions)\n",
    "    \n",
    "average_test_predictions = np.mean(all_test_predictions, axis=0)\n",
    "predicted_classes_index = np.argmax(average_test_predictions, axis=1)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "print(\"average_test_predictions:\", average_test_predictions)\n",
    "print(\"predicted_classes_index:\", predicted_classes_index)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "# 所有模型的預測結果的平均\n",
    "\n",
    "\n",
    "\n",
    "true_classes_index = np.argmax([np.array(label) for label in oct_label_one_hot], axis=1)\n",
    "# print(true_classes_index)\n",
    "\n",
    "# print(f\"({len(all_test_predictions)}, {len(all_test_predictions[0])}, {len(all_test_predictions[0][0])})\")\n",
    "# print(average_test_predictions.shape)\n",
    "# print(predicted_classes_index.shape)\n",
    "# print(average_test_predictions)\n",
    "\n",
    "# # 計算confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes_index, predicted_classes_index, labels=np.arange(3))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(true_classes_index, predicted_classes_index)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec05ca-7ddb-44e8-a4c8-a65e62f65414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
